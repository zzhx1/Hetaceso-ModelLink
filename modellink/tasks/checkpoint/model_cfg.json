{
  "model_mappings": {
    "base": {
      "config_set_value": {
        "seq_length": 4096,
        "global_batch_size": 1024,
        "add_position_embedding": true,
        "use_rotary_position_embeddings": true,
        "add_bias_linear": false,
        "gradient_accumulation_fusion": false,
        "normalization": "RMSNorm",
        "swiglu": true,
        "moe_flag": false,
        "tokenizer_type": "Llama2Tokenizer",
        "group_query_attention": false,
        "qkv_type": "unpack",
        "fc_type": "gate_up_down"
      },
      "config_hf_key_mapping": {
        "max_position_embeddings": "max_position_embeddings",
        "hidden_size": "hidden_size",
        "num_attention_heads": "num_attention_heads",
        "num_layers": "num_hidden_layers",
        "num_key_value_heads": "num_key_value_heads",
        "vocab_size": "vocab_size",
        "intermediate_size": "intermediate_size",
        "norm_epsilon": "rms_norm_eps",
        "tie_word_embeddings": "tie_word_embeddings",
		    "torch_dtype": "torch_dtype"
      },
      "model_hf_key_mapping": {
        "model": "module[0]",
        "embedding_word_embeddings": "model.embed_tokens",
        "embedding_word_embeddings_norm": "model.embedding.word_embeddings.norm",
        "layers": "model.layers",
        "layers_input_layernorm": "model.layers[layer_idx].input_layernorm",
        "layers_self_attention_linear_proj": "model.layers[layer_idx].self_attn.o_proj",
        "layers_self_attention_linear_q_proj": "model.layers[layer_idx].self_attn.q_proj",
        "layers_self_attention_linear_k_proj": "model.layers[layer_idx].self_attn.k_proj",
        "layers_self_attention_linear_v_proj": "model.layers[layer_idx].self_attn.v_proj",
        "layers_self_attention_pre_mlp_layernorm": "model.layers[layer_idx].post_attention_layernorm",
        "layers_mlp_gate_proj": "model.layers[layer_idx].mlp.gate_proj",
        "layers_mlp_up_proj": "model.layers[layer_idx].mlp.up_proj",
        "layers_mlp_linear_fc2": "model.layers[layer_idx].mlp.down_proj",
        "final_layernorm": "model.norm",
        "output_layer": "lm_head"
      }
    },
    "llama2": {
      "__base__": "base"
    },
    "baichuan": {
      "__base__": "base",
      "config_set_value": {
        "qkv_type": "pack_gqa"
      },
      "model_hf_key_mapping": {
        "layers_self_attention_linear_qkv_pack": "model.layers[layer_idx].self_attn.W_pack"
      }
    },
    "chatglm3": {
      "__base__": "base",
      "config_set_value": {
        "seq_length": 32768,
        "global_batch_size": 16,
        "group_query_attention": true,
        "qkv_type": "pack_gqa",
        "fc_type": "h_to_4h"
      },
      "config_hf_key_mapping": {
        "max_position_embeddings": "seq_length",
        "num_layers": "num_layers",
        "num_key_value_heads": "multi_query_group_num",
        "vocab_size": "padded_vocab_size",
        "intermediate_size": "ffn_hidden_size",
        "norm_epsilon": "layernorm_epsilon"
      },
      "model_hf_key_mapping": {
        "embedding_word_embeddings": "transformer.embedding.word_embeddings",
        "layers": "transformer.encoder.layers",
        "layers_input_layernorm": "transformer.encoder.layers[layer_idx].input_layernorm",
        "layers_self_attention_linear_qkv_pack": "transformer.encoder.layers[layer_idx].self_attention.query_key_value",
        "layers_self_attention_linear_proj": "transformer.encoder.layers[layer_idx].self_attention.dense",
        "layers_self_attention_pre_mlp_layernorm": "transformer.encoder.layers[layer_idx].post_attention_layernorm",
        "layers_mlp_linear_fc1": "transformer.encoder.layers[layer_idx].mlp.dense_h_to_4h",
        "layers_mlp_linear_fc2": "transformer.encoder.layers[layer_idx].mlp.dense_4h_to_h",
        "final_layernorm": "transformer.encoder.final_layernorm",
        "output_layer": "transformer.output_layer"
      }
    },
    "mixtral": {
      "__base__": "base",
	  "config_set_value": {
		  "moe_flag": true
	  },
	  "model_hf_key_mapping": {
		  "layers_mlp_router": "model.layers[layer_idx].block_sparse_moe.gate",
		  "layers_mlp_gate_proj": "model.layers[layer_idx].block_sparse_moe.experts[expert_idx].w1",
          "layers_mlp_up_proj": "model.layers[layer_idx].block_sparse_moe.experts[expert_idx].w3",
          "layers_mlp_linear_fc2": "model.layers[layer_idx].block_sparse_moe.experts[expert_idx].w2"
	  }
    },
    "gemma": {
      "__base__": "base",
      "config_set_value": {
        "seq_length": 8192,
        "tie_word_embeddings": true,
        "kv_channels": 256
      }
    },
    "gemma2": {
      "__base__": "base",
      "config_set_value": {
        "seq_length": 8192,
        "tie_word_embeddings": true
	  },
      "config_hf_key_mapping": {
        "kv_channels": "head_dim"
      },
      "model_hf_key_mapping": {
        "layers_self_attention_post_attention_layernorm": "model.layers[layer_idx].post_attention_layernorm",
        "layers_self_attention_pre_mlp_layernorm": "model.layers[layer_idx].pre_feedforward_layernorm",
        "layers_self_attention_post_mlp_layernorm": "model.layers[layer_idx].post_feedforward_layernorm"
      }
    },
    "bloom": {
      "__base__": "base",
      "config_set_value": {
        "normalization": "LayerNorm",
        "fc_type": "h_to_4h",
        "add_bias_linear": true,
        "swiglu": false,
        "tie_word_embeddings": true,
        "seq_length": 2048,
        "max_position_embeddings": 2048,
        "intermediate_size": 16384,
        "embed_layernorm": true,
        "norm_has_bias": true
      },
      "config_hf_key_mapping": {
        "num_layers": "n_layer",
        "num_key_value_heads": "n_head",
        "norm_epsilon": "layer_norm_epsilon",
        "num_attention_heads": "n_head",
        "max_position_embeddings": "hidden_size",
        "intermediate_size": "hidden_size"
      },
      "model_hf_key_mapping": {
        "embedding_word_embeddings": "transformer.word_embeddings",
        "embedding_word_embeddings_norm": "transformer.word_embeddings_layernorm",
        "layers": "transformer.h",
        "layers_input_layernorm": "transformer.h[layer_idx].input_layernorm",
        "layers_self_attention_linear_qkv": "transformer.h[layer_idx].self_attention.query_key_value",
        "layers_self_attention_linear_proj": "transformer.h[layer_idx].self_attention.dense",
        "layers_self_attention_pre_mlp_layernorm": "transformer.h[layer_idx].post_attention_layernorm",
        "layers_mlp_linear_fc1": "transformer.h[layer_idx].mlp.dense_h_to_4h",
        "layers_mlp_linear_fc2": "transformer.h[layer_idx].mlp.dense_4h_to_h",
        "final_layernorm": "transformer.ln_f",
        "output_layer": "lm_head"
      }
    },
    "qwen": {
      "__base__": "base",
      "config_set_value": {
        "tokenizer_type": "PretrainedFromHF",
        "intermediate_size": 11008,
        "qkv_type": "pack_gqa"
      },
      "config_hf_key_mapping": {
        "norm_epsilon": "layer_norm_epsilon",
        "num_key_value_heads": "num_attention_heads"
      },
      "model_hf_key_mapping": {
        "embedding_word_embeddings": "transformer.wte",
        "layers_input_layernorm": "transformer.h[layer_idx].ln_1",
        "layers_self_attention_linear_proj": "transformer.h[layer_idx].attn.c_proj",
        "layers_self_attention_linear_qkv_pack": "transformer.h[layer_idx].attn.c_attn",
        "layers_self_attention_pre_mlp_layernorm": "transformer.h[layer_idx].ln_2",
        "layers_mlp_gate_proj": "transformer.h[layer_idx].mlp.w2",
        "layers_mlp_up_proj": "transformer.h[layer_idx].mlp.w1",
        "layers_mlp_linear_fc2": "transformer.h[layer_idx].mlp.c_proj",
        "final_layernorm": "transformer.ln_f"
      }
    },
    "internlm2": {
      "__base__": "base",
      "config_set_value": {
        "seq_length": 32768,
        "group_query_attention": true,
        "qkv_type": "pack_self"
      },
      "model_hf_key_mapping": {
        "model": "module[tp_rank]",
        "embedding_word_embeddings": "model.tok_embeddings",
        "layers": "model.layers",
        "layers_self_attention_linear_qkv_pack": "model.layers[layer_idx].attention.wqkv",
        "layers_self_attention_linear_proj": "model.layers[layer_idx].attention.wo",
        "layers_mlp_gate_proj": "model.layers[layer_idx].feed_forward.w1",
        "layers_mlp_up_proj": "model.layers[layer_idx].feed_forward.w3",
        "layers_mlp_linear_fc2": "model.layers[layer_idx].feed_forward.w2",
        "layers_input_layernorm": "model.layers[layer_idx].attention_norm",
        "layers_self_attention_pre_mlp_layernorm": "model.layers[layer_idx].ffn_norm",
        "final_layernorm": "model.norm",
        "output_layer": "output"
      }
    },
    "deepseek2": {
        "__base__": "base",
        "config_set_value": {
          "seq_length": 8192,
          "global_batch_size": 64,
          "qkv_type": "pack_mla",
          "multi_head_latent_attention": true,
          "mlp_experts_flag": true,
          "qk_layernorm": true
        },
        "config_hf_key_mapping": {
          "first_k_dense_replace": "first_k_dense_replace",
          "kv_lora_rank": "kv_lora_rank",
          "moe_intermediate_size": "moe_intermediate_size",
          "moe_layer_freq": "moe_layer_freq",
          "num_experts": "n_routed_experts",
          "n_shared_experts": "n_shared_experts",
          "q_lora_rank": "q_lora_rank",
          "qk_nope_head_dim": "qk_nope_head_dim",
          "qk_rope_head_dim": "qk_rope_head_dim"
        },
        "model_hf_key_mapping": {
          "layers_self_attention_linear_q_proj": "model.layers[layer_idx].self_attn.q_a_proj",
          "layers_self_attention_linear_kv_proj": "model.layers[layer_idx].self_attn.kv_a_proj_with_mqa",
          "layers_self_attention_linear_qb": "model.layers[layer_idx].self_attn.q_b_proj",
          "layers_self_attention_linear_kvb": "model.layers[layer_idx].self_attn.kv_b_proj",
          "layers_self_attention_q_layernorm": "model.layers[layer_idx].self_attn.q_a_layernorm",
          "layers_self_attention_k_layernorm": "model.layers[layer_idx].self_attn.kv_a_layernorm",
          "layers_mlp_router": "model.layers[layer_idx].mlp.gate",
          "layers_mlp_experts_gate_proj": "model.layers[layer_idx].mlp.experts[expert_idx].gate_proj",
          "layers_mlp_experts_up_proj": "model.layers[layer_idx].mlp.experts[expert_idx].up_proj",
          "layers_mlp_experts_linear_fc2": "model.layers[layer_idx].mlp.experts[expert_idx].down_proj",
          "layers_mlp_shared_experts_gate_proj": "model.layers[layer_idx].mlp.shared_experts.gate_proj",
          "layers_mlp_shared_experts_up_proj": "model.layers[layer_idx].mlp.shared_experts.up_proj",
          "layers_mlp_shared_experts_linear_fc2": "model.layers[layer_idx].mlp.shared_experts.down_proj"
        }
    },
    "minicpm": {
      "__base__": "base",
      "config_set_value": {
        "tie_word_embeddings": true
      }
    },
    "minicpm-moe": {
      "__base__": "base",
      "config_set_value": {
        "moe_flag": true,
        "tie_word_embeddings": true
      },
      "model_hf_key_mapping": {
        "embedding_word_embeddings": "model.embed_tokens",
        "layers": "model.layers",
        "layers_input_layernorm": "model.layers[layer_idx].input_layernorm",
        "layers_self_attention_linear_proj": "model.layers[layer_idx].self_attn.o_proj",
        "layers_self_attention_linear_q_proj": "model.layers[layer_idx].self_attn.q_proj",
        "layers_self_attention_linear_k_proj": "model.layers[layer_idx].self_attn.k_proj",
        "layers_self_attention_linear_v_proj": "model.layers[layer_idx].self_attn.v_proj",
        "layers_self_attention_pre_mlp_layernorm": "model.layers[layer_idx].post_attention_layernorm",
        "layers_mlp_router": "model.layers[layer_idx].mlp.gate",
        "layers_mlp_gate_proj": "model.layers[layer_idx].mlp.experts[expert_idx].w1",
        "layers_mlp_up_proj": "model.layers[layer_idx].mlp.experts[expert_idx].w3",
        "layers_mlp_linear_fc2": "model.layers[layer_idx].mlp.experts[expert_idx].w2",
        "final_layernorm": "model.norm"
      }
    },
    "baichuan2": {
      "__base__": "base",
      "config_set_value": {
        "qkv_type": "pack_gqa",
        "max_position_embeddings": 4096
      },
      "model_hf_key_mapping": {
        "layers_self_attention_linear_qkv_pack": "model.layers[layer_idx].self_attn.W_pack"
      }
    }
  }
}
