{
  "model_mappings": {
    "base": {
      "config_set_value": {
        "seq_length": 4096,
        "global_batch_size": 1024,
        "add_position_embedding": true,
        "use_rotary_position_embeddings": true,
        "add_bias_linear": false,
        "gradient_accumulation_fusion": false,
        "normalization": "RMSNorm",
        "swiglu": true,
        "moe_flag": false,
        "tokenizer_type": "Llama2Tokenizer",
        "group_query_attention": false,
        "qkv_type": "unpack"
      },
      "config_hf_key_mapping": {
        "max_position_embeddings": "max_position_embeddings",
        "hidden_size": "hidden_size",
        "num_attention_heads": "num_attention_heads",
        "num_layers": "num_hidden_layers",
        "num_key_value_heads": "num_key_value_heads",
        "vocab_size": "vocab_size",
        "intermediate_size": "intermediate_size",
        "norm_epsilon": "rms_norm_eps",
        "tie_word_embeddings": "tie_word_embeddings"
      },
      "model_hf_key_mapping": {
        "model": "module[tp_rank]",
        "embedding_word_embeddings": "module[tp_rank].model.embed_tokens",
        "embedding_word_embeddings_norm": "module[tp_rank].model.embedding.word_embeddings.norm",
        "layers": "module[tp_rank].model.layers",
        "layers_input_layernorm": "module[tp_rank].model.layers[layer_idx].input_layernorm",
        "layers_self_attention_linear_proj": "module[tp_rank].model.layers[layer_idx].self_attn.o_proj",
        "layers_self_attention_linear_q_proj": "module[tp_rank].model.layers[layer_idx].self_attn.q_proj",
        "layers_self_attention_linear_k_proj": "module[tp_rank].model.layers[layer_idx].self_attn.k_proj",
        "layers_self_attention_linear_v_proj": "module[tp_rank].model.layers[layer_idx].self_attn.v_proj",
        "layers_self_attention_pre_mlp_layernorm": "module[tp_rank].model.layers[layer_idx].post_attention_layernorm",
        "layers_mlp_gate_proj": "module[tp_rank].model.layers[layer_idx].mlp.gate_proj",
        "layers_mlp_up_proj": "module[tp_rank].model.layers[layer_idx].mlp.up_proj",
        "layers_mlp_linear_fc2": "module[tp_rank].model.layers[layer_idx].mlp.down_proj",
        "final_layernorm": "module[tp_rank].model.norm",
        "output_layer": "module[tp_rank].lm_head"
      }
    },
    "llama2": {
      "__base__": "base"
    }
  }
}
