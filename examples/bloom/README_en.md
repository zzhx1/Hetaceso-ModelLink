# Bloom
<p align="left">
        <b><a href="README.md">简体中文</a></b> |
        <b>English</b> 
    </p>
</p>

[TOC]


# Bloom-7B

## Training

Here's a hardware summary of pre-training Bloom-7B:

| Hardware |                      Value                      |
| :------: | :---------------------------------------------: |
|   NPU    |               8 x Ascend NPUs                   |

### Script

1. Clone the repository to your local server:
```shell
git clone https://gitee.com/ascend/ModelLink.git 
cd ModeLlink 
mkdir logs
mkdir ckpt
```

2. Build environment

```bash
# python3.8
conda create -n test python=3.8
conda activate test

# install torch and torch_npu 
pip install torch-2.1.0-cp38-cp38m-linux_aarch64.whl
pip install torch_npu-2.1.0.XXX-cp38-cp38m-linux_aarch64.whl
pip install apex-0.1_ascend*-cp38-cp38m-linux_aarch64.whl

# modify the path according to your own  ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# install AscendSpeed
git clone https://gitee.com/ascend/AscendSpeed.git
cd AscendSpeed
pip install -r requirements.txt 
pip3 install -e .
cd ..

# install other packages
pip install -r requirements.txt 
```

3. Prepare pretrained weights
Download the Bloom-7B checkpoint from [here](https://huggingface.co/bigscience/bloom-7b1/tree/main)

```shell
mkdir tokenizer
cd tokenizer
wget https://huggingface.co/bigscience/bloom/resolve/main/special_tokens_map.json
wget https://huggingface.co/bigscience/bloom/resolve/main/tokenizer.json
wget https://huggingface.co/bigscience/bloom/resolve/main/tokenizer_config.json
...
cd ..
```

4. Weights convert

HuggingFace weights --> Megatron weights
***(This scenario is generally used to train open-source HuggingFace models on Megatron)***

```shell
python tools/checkpoint/convert_ckpt.py --model-type GPT \
                                --loader loader_bloom_hf \
                                --saver saver_megatron \
                                --target-tensor-parallel-size 8 \
                                --target-pipeline-parallel-size 1 \
                                --load-dir /bloom-7b \
                                --save-dir /{your save dir} \
                                --tokenizer-model None 
```

Any Megatron weights with parallel slicing strategy --> Any Megatron weights with parallel slicing strategy
***(This scenario is generally used to convert the trained megatron model back to the HuggingFace format)***
```shell
cd ModelLink/
# Modify the ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh
python tools/checkpoint/convert_ckpt.py --model-type GPT \
    --loader megatron \
    --saver megatron \
    --save-model-type save_huggingface_llama \
    --load-dir ../HF_Bloom7B-v0.1-pt8-pp1 \
    --target-tensor-parallel-size 1 \
    --target-pipeline-parallel-size 1 \
    --embed-layernorm \
    --save-dir ../HF_Bloom7B_downloaded   # <-- Fill in the original HF model path here, new weights will be saved in ../HF_Bloom7B_downloaded/mg2hg
```

5. Prepare dataset

Download the Bloom-7B datasets from [here](https://huggingface.co/datasets/tatsu-lab/alpaca/resolve/main/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet)

```shell
# download datasets
 mkdir dataset_bloom7b
     cd ./dataset_bloom7b
     wget https://huggingface.co/datasets/tatsu-lab/alpaca/resolve/main/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet
     cd ..
     cd ModelLink
```

```shell
# prepare datasets
python ./tools/preprocess_data.py \
       --input ../dataset_bloom7b/train-00000-of-00001-a09b74b3ef9c3b56.parquet \
       --tokenizer-name-or-path ../bloom-7b-hf \
       --output-prefix ../dataset_bloom7b/alpaca \
       --workers 4 \
       --log-interval 1000 \
       --tokenizer-type PretrainedFromHF
    cd .. 
```

6. Config Bloom-7B pre-training script(Bloom-7B does not support Flash Attention) : examples/bloom/pretrain_bloom_ptd_7B.sh 

```shell
# modify the script according to your own  ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh 

CKPT_SAVE_DIR="./ckpt"
DATA_PATH="./dataset_bloom-7B/bigscience_bloom-7b1_master_text_document"
TOKENIZER_PATH="./bloom-7B-hf/"
CKPT_LOAD_DIR="./bloom-7b"
```


7. Launch Bloom-7B  pre-training script: examples/bloom/pretrain_bloom_ptd_7B.sh 

```shell
bash examples/bloom/pretrain_bloom_ptd_7B.sh 
```
**Note**: If using multi machine training, it is necessary to set up multi machine data sharing, and non primary nodes can read the primary node data through data sharing. Alternatively, directly copy the data generated by the master node to non master nodes.


### Performance

#### Machine performance

The performance of Bloom-7B in **Ascend NPU** and **Reference**:

| Device | Model       | total Iterations | throughput rate (samples/s) | throughput rate (tokens/s/p) | single-step time (s/step) | 
|:----:|:---------:|:----:|:---------------------:|:---------------:|:----------------:|
| NPUs | Bloom 7b | 1000 | 7.95 | 2034 | 64.55
|  Reference  | Bloom 7B | 1000 | 9.894 |  2525 | 19.40 



## Inference Bloom-7B
Config Bloom-7B inference script: examples/bloom/generate_bloom_7b_ptd.sh
```bash
# modify the script according to your own ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh 
 
# modify script model path and tokenizer path
CHECKPOINT="your model save ckpt path"
TOKENIZER_PATH="your tokenizer path"
```
Launch Bloom-7B inference script: examples/bloom/generate_bloom_7b_ptd.sh
```bash
bash examples/bloom/generate_bloom_7b_ptd.sh
```
Some inference samples are as follows:

![Inference](../../sources/images/bloom/bloom7b-generate.png)

## Evaluation Bloom-7B
Config Bloom-7B evaluation script: examples/bloom/evaluate_bloom_7B_ptd.sh

```bash
source /usr/local/Ascend/ascend-toolkit/set_env.sh 

# modify script model path and tokenizer path
CHECKPOINT="your model save ckpt path"
TOKENIZER_PATH="your tokenizer path"
# configure task and data path
DATA_PATH="your dataset path"
TASK="your task"
```
Launch Bloom-7B evaluation script:

```bash
bash examples/bloom/evaluate_bloom_7B_ptd.sh
```

Evaluation results

|  dataset | subject_num | question_num | reference_acc |NPU acc|
|:---:|:-----------:|:------------:|:-------------:|:---:|
| MMLU |     57      |    14042     |    0.254    |0.251|



# Bloom-176B

## Training

BLOOM model is from: [A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100).

> Scao, Teven Le, et al. "Bloom: A 176b-parameter open-access multilingual language model." arXiv preprint arXiv:2211.05100 (2022).

Here's a hardware summary of pre-training Bloom-176B:

| **Hardware** | **Value**       |
| ------------ | --------------- |
| NPU          | 12x8 Ascend 910 |


## Script

1. Clone the repository to your local server

```shell
git clone https://gitee.com/ascend/ModelLink.git
cd ModelLink
mkdir logs
mkdir ckpt
```

2. Build enviroment

```bash
# python3.8
conda create -n test python=3.8
conda activate test

# install torch and torch_npu 
pip install torch-2.1.0-cp38-cp38m-linux_aarch64.whl
pip install torch_npu-2.1.0.XXX-cp38-cp38m-linux_aarch64.whl
pip install apex-0.1_ascend*-cp38-cp38m-linux_aarch64.whl

# modify the path according to your own  ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# install AscendSpeed
git clone https://gitee.com/ascend/AscendSpeed.git
cd AscendSpeed
pip install -r requirements.txt 
pip3 install -e .
cd ..

# install other packages
pip install -r requirements.txt 

3. Prepare pretrained weights

Download the Bloom-176B tokensizer from [here](https://huggingface.co/bigscience/bloom/tree/main).

```shell
mkdir tokenizer
cd tokenizer
wget https://huggingface.co/bigscience/bloom/resolve/main/special_tokens_map.json
wget https://huggingface.co/bigscience/bloom/resolve/main/tokenizer.json
wget https://huggingface.co/bigscience/bloom/resolve/main/tokenizer_config.json
cd ..
```

4. Weights convert

HuggingFace weights --> Megatron weights
***(This scenario is generally used to train open-source HuggingFace models on Megatron)***

```shell
python tools/checkpoint/convert_ckpt.py --model-type GPT \
                                --loader loader_bloom_hf \
                                --saver saver_megatron \
                                --target-tensor-parallel-size 8 \
                                --target-pipeline-parallel-size 5 \
                                --load-dir /bloom-176b \
                                --save-dir /{your save dir} \
                                --tokenizer-model None \
                                --params-dtype bf16 
```

Any Megatron weights with parallel slicing strategy --> Any Megatron weights with parallel slicing strategy
***(This scenario is generally used to convert the trained megatron model back to the HuggingFace format)***
```shell
cd ModelLink/
# Modify the ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh
python tools/checkpoint/convert_ckpt.py --model-type GPT \
    --loader megatron \
    --saver megatron \
    --save-model-type save_huggingface_llama \
    --load-dir ../HF_Bloom176B-v0.1-pt8-pp1 \
    --target-tensor-parallel-size 1 \
    --target-pipeline-parallel-size 1 \
    --embed-layernorm \
    --params-dtype bf16 \
    --save-dir ../HF_Bloom176B_downloaded   # <-- Fill in the original HF model path here, new weights will be saved in ../HF_Bloom176B_downloaded/mg2hg
```

5. Prepare dataset

Download the bloom-176b datasets from [here](https://huggingface.co/datasets/tatsu-lab/alpaca/resolve/main/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet)    
```shell
# download datasets
mkdir dataset_bloom176b
     cd ./dataset_bloom176b
     wget https://huggingface.co/datasets/tatsu-lab/alpaca/resolve/main/data/train-00000-of-00001-a09b74b3ef9c3b56.parquet
     cd ..
     cd ModelLink
	
# process datasets                              
python ./tools/preprocess_data.py \
       --input ../dataset_bloom176b/train-00000-of-00001-a09b74b3ef9c3b56.parquet \
       --tokenizer-name-or-path ../bloom-176b-hf \
       --output-prefix ../dataset_bloom176b/alpaca \
       --workers 4 \
       --log-interval 1000 \
       --tokenizer-type PretrainedFromHF
    cd .. 
```
6. Config Bloom-176B pre-training script(Bloom-176B does not support Flash Attention): examples/bloom/pretrain_bloom_176b.sh

```shell
# modify MASTER_ADDR to the IP address of the master node in the cluster.
# the master node is localhost, and the other nodes are the IP address of the master node
MASTER_ADDR=localhost

# modify the rank number of a node. The rank number of the master node is 0, and the rank number of other nodes increases in ascending order.
NODE_RANK=0

# modify the datasets path and tokenizer path
TOKENIZER_NAME_OR_PATH=/home/bloom_data/vocab_file/
DATA_PATH=/home/bloom_data/enwiki_100k/enwiki-100k_text_document
```

7. Launch Bloom-176B pre-training script: examples/bloom/pretrain_bloom_176b.sh

Run the examples/bloom/pretrain_bloom_176b.sh on all nodes in the cluster.

```shell
bash examples/bloom/pretrain_bloom_176b.sh
```
**Note**: If using multi machine training, it is necessary to set up multi machine data sharing, and non primary nodes can read the primary node data through data sharing. Alternatively, directly copy the data generated by the master node to non master nodes.

## Performance

### Machine Performance

The performance of Bloom-176B in **Ascend NPU** and **Reference**:

| Devices   | Model      | total iterations | throughput rate (tokens/s/p) |
| --------- | ---------- | ---------------- | ---------------------------- |
| NPUs      | Bloom-176B | 1000             | 100                         |
| Reference | Bloom-176B | NA               | 107                          |


## Inference Bloom 176B
Config Bloom-176B inference script: examples/bloom/generate_bloom_176b_ptd.sh
```bash
# modify the script according to your own ascend-toolkit path
source /usr/local/Ascend/ascend-toolkit/set_env.sh 
 
# modify script model path and tokenizer path
CHECKPOINT="your model save ckpt path"
TOKENIZER_PATH="your tokenizer path"
```
Launch Bloom-176B inference script: examples/bloom/generate_bloom_176b_ptd.sh

Bloom-176b needs 5 machines to inference, so you need to convert a new model, set
tp=8, pp=5
```bash
bash examples/bloom/generate_bloom_176b_ptd.sh
```
Some inference samples are as follows:

![Inference](../../sources/images/bloom/bloom176b-generate.png)

## Evaluation Bloom 176B
Config Bloom-176B evaluation script: examples/bloom/evaluate_bloom_176B_ptd.sh

```bash
source /usr/local/Ascend/ascend-toolkit/set_env.sh 

# modify script model path and tokenizer path
CHECKPOINT="your model save ckpt path"
TOKENIZER_PATH="your tokenizer path"
# configure task and data path
DATA_PATH="your dataset path"
TASK="your task"
```
Launch Bloom-176B evaluation script:

```bash
bash examples/bloom/evaluate_bloom_176B_ptd.sh
```

Evaluation results

|  dataset |reference_acc |NPU acc|
|:---:|:-------------:|:---:|
| boolq |  /    |0.645|
